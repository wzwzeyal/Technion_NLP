name: Tweet4.0 # Experiment name to save in the logger
seed: 42  # Random seed for reproducibility
num_epochs: 30   # Number of epochs to train the model
learning_rate: 0.0001
accumulation_steps: 1
do_train: true
do_eval_on_train: true  # Run evaluation on train set in the end of every epoch
do_eval: true  # Evaluate on dev set at the end of every epoch
do_test: false  # Evaluate on test set at the end of training
seq_model_name: GRU

data_args:
  max_seq_length: 64
  batch_size: 2048
  shuffle: True
  eval_batch_size: 16  # Batch size for evaluation
  minimum_vocab_freq_threshold: 1

model_args:
  output_size: 5  # Should correspond to the number of classes
  dropout: 0.2  # Dropout of the final classifier in the model
  seq_args:
    hidden_size: 256  # Size of the hidden state of the LSTM
    bidirectional: false
    input_size: 50  # Size of the input to the LSTM
    num_layers: 3
    bias: true
    batch_first: true
    dropout: 0.2

#    proj_size: 0

# version 0.1