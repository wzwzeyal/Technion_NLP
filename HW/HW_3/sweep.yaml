program: train.py
project: IMDB_Tutorial
method: bayes

metric:
  name: dev_accuracy
  goal: maximize
parameters:
  accumulation_steps:
    values: [ 1, 2, 4 ]
  batch_size:
    values: [ 8, 16 ]
  minimum_vocab_freq_threshold:
    values: [0, 1]
  learning_rate:
    values: [0.001, 0.0001, 0.00001]
  backbone_model:
    values: [ "RNN", "GRU", "LSTM" ]
  cat_max_and_mean:
    values: [ "False", "True" ]
  dropout:
    values: [ 0.2 ]
  embedding:
    values: [ "none", "glove-wiki-gigaword" ]
  embedding_weight_requires_grad:
    values: [ "False", "True"]
  bidirectional:
    values: [ "False", "True"]
  hidden_size:
    values: [ 256, 512 ]
  input_size:
    values: [ 50, 100 ]
  num_layers:
    values: [3]

# Tuning








# extra params




#{'accumulation_steps': 1,
# 'data_args': {'batch_size': 8,
#               'eval_batch_size': 1,
#               'max_seq_length': 64,
#               'minimum_vocab_freq_threshold': 0,
#               'shuffle': False},
# 'do_eval': True,
# 'do_eval_on_train': True,
# 'do_infer': False,
# 'do_test': False,
# 'do_train': True,
# 'learning_rate': 0.0001,
# 'model_args': {'backbone_model': 'LSTM',
#                'cat_max_and_mean': False,
#                'dropout': 0.2,
#                'embedding': 'none',
#                'embedding_weight_requires_grad': False,
#                'output_size': 5,
#                'seq_args': {'batch_first': True,
#                             'bias': True,
#                             'bidirectional': True,
#                             'dropout': 0.2,
#                             'hidden_size': 256,
#                             'input_size': 50,
#                             'num_layers': 3}},
# 'name': 'run',
# 'num_epochs': 15,
# 'seed': 42}
